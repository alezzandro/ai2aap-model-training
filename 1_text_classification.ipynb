{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upgrade pip and install all the required libraries\n",
    "! pip install --upgrade pip\n",
    "! pip install torch==2.3.0+cpu accelerate -f https://download.pytorch.org/whl/torch_stable.html\n",
    "! pip install tf_keras tensorflow datasets evaluate transformers einops bitsandbytes sentence_transformers ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check git-lfs is available\n",
    "\n",
    "If we are using this notebook in a workbench in OpenShift AI, we may need to download git-lfs binary to work with large models and HugginFace.\n",
    "\n",
    "**Please note**: the check is really basic, it just tries running the git-lfs binary, so if you have it installed just discard the output of the following code line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download git-lfs and extract it in the \"bin\" directory\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "def download_and_extract(url, target_path, extract_dir):\n",
    "    \"\"\"Downloads a tar.gz file and extracts its contents.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the tar.gz file.\n",
    "        target_path (str): Path where the downloaded file will be saved.\n",
    "        extract_dir (str): Directory where the contents will be extracted.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url, stream=True)  # Stream for large files\n",
    "    response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "    with open(target_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    print(f\"Downloaded to {target_path}\")\n",
    "\n",
    "    with tarfile.open(target_path, 'r:gz') as tar:\n",
    "        tar.extractall(extract_dir)\n",
    "\n",
    "    print(f\"Extracted to {extract_dir}\")\n",
    "\n",
    "if os.system(\"git-lfs\") != 0:\n",
    "    # Clean up any previous download\n",
    "    try:\n",
    "        shutil.rmtree(\"./bin\")\n",
    "        shutil.rmtree(\"./git-lfs-3.5.1\")\n",
    "        os.remove(\"git-lfs-linux-amd64-v3.5.1.tar.gz\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    # Our variabiles\n",
    "    url = \"https://github.com/git-lfs/git-lfs/releases/download/v3.5.1/git-lfs-linux-amd64-v3.5.1.tar.gz\"\n",
    "    target_path = \"git-lfs-linux-amd64-v3.5.1.tar.gz\"\n",
    "    extract_dir = \"./\"\n",
    "\n",
    "    # Download the file and extract it\n",
    "    download_and_extract(url, target_path, extract_dir)\n",
    "\n",
    "    # Create the ./bin directory \n",
    "    os.mkdir(\"./bin/\")\n",
    "\n",
    "    # Move the git-lfs binary in the ./bin directory\n",
    "    shutil.move(\"./git-lfs-3.5.1/git-lfs\", \"./bin/\")\n",
    "\n",
    "    # Add ./bin directory to the PATH env variable\n",
    "    current_path = os.environ.get('PATH', '')\n",
    "    new_path = os.path.abspath(os.getcwd())+\"/bin\"+\":\"+current_path\n",
    "\n",
    "    os.environ['PATH'] = new_path  # Update PATH in the current Python process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose if you want to Login to your Hugging Face account so you can upload and share your model with the community. First of all choose if you want to login and if yes When prompted, please enter your token to login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from huggingface_hub import login\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=[\"True\", \"False\"],\n",
    "    value=\"False\",\n",
    "    description='Choose: ',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "login2hf = (dropdown.value == 'True')\n",
    "\n",
    "if login2hf:\n",
    "    hf_token = input(\"Please enter your HuggingFace token: \")\n",
    "    login(token=hf_token, write_permission=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Tickets dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the Tickets dataset from local repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# I've created an HuggingFace dataset, but please refer to the followin (commented) line in case you want to load data from local csv files:\n",
    "# tickets = load_dataset(\"csv\", data_files={\"train\": \"900tickets-300WS-300DB-300FS-train.csv\", \"test\": \"900tickets-300WS-300DB-300FS-test.csv\"})\n",
    "\n",
    "# Replace the dataset name with yours if you have any\n",
    "tickets = load_dataset(\"alezzandro/itsm_tickets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load a DistilBERT tokenizer to preprocess the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preprocessing function to tokenize the text and truncate sequences to be no longer than DistilBERT's maximum input length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_tickets = tickets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the ðŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start training your model, create a map of the expected ids to their labels with `id2label` and `label2id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"WebServer\", 1: \"Database\", 2: \"Filesystem\"}\n",
    "label2id = {\"WebServer\": 0, \"Database\": 1, \"Filesystem\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "You're ready to start training your model now! Load DistilBERT with [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) along with the number of expected labels, and the label mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, only three steps remain:\n",
    "\n",
    "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\n",
    "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/itsm_tickets\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    hub_model_id=\"alezzandro/itsm_tickets\",\n",
    "    push_to_hub=login2hf,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_tickets[\"train\"],\n",
    "    eval_dataset=tokenized_tickets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) applies dynamic padding by default when you pass `tokenizer` to it. In this case, you don't need to specify a data collator explicitly.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if login2hf:\n",
    "    trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that you've finetuned a model, you can use it!\n",
    "\n",
    "Grab some text you'd like to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Web server logs indicate multiple 404 Not Found errors for resources that should exist.  File paths appear correct in the codebase. Need to investigate potential caching issues, configuration mismatches, or incorrect deployments.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for sentiment analysis with your model, and pass your text to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# You should replace the name of the model or load it from local directory in case you changed something.\n",
    "classifier = pipeline(\"text-classification\", model=\"alezzandro/itsm_tickets\")\n",
    "result = classifier(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the model to OpenVINO format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's download the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install openvino\n",
    "! pip install -q \"git+https://github.com/huggingface/optimum-intel.git\" onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are ready to start the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForSequenceClassification\n",
    "# If you are using a local model, replace the model name with yours or the directory path:\n",
    "model = OVModelForSequenceClassification.from_pretrained(\"alezzandro/itsm_tickets\", export=True)\n",
    "\n",
    "os.makedirs(\"./models2s3/itsm_tickets_ovir\")\n",
    "\n",
    "model.save_pretrained(\"./models2s3/itsm_tickets_ovir\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
